# -*- coding: utf-8 -*-
"""CLIP_BLIP_handson.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wuyb14OoDoa9TU4BstDPgQlAKSnGRiMd
"""

!pip install torch torchvision ftfy regex tqdm transformers diffusers
!pip install git+https://github.com/openai/CLIP.git
!pip install git+https://github.com/salesforce/BLIP.git

from diffusers import StableDiffusionPipeline
import torch

# Load Stable Diffusion model
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe.to("cuda" if torch.cuda.is_available() else "cpu")

# Define a text prompt for image generation
prompt = "A cat sitting on a couch"
generated_image = pipe(prompt).images[0]
display(generated_image)
# Save and display the generated image
generated_image.save("generated_image.png")
generated_image.show()

display(generated_image.resize((256, 256)))

import clip
from PIL import Image

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load the generated image
image = preprocess(Image.open("generated_image.png")).unsqueeze(0).to(device)

# Define possible text descriptions
text_descriptions = ["A cat sitting on a couch", "A dog playing in a park", "A person reading a book"]
text_tokens = clip.tokenize(text_descriptions).to(device)

# Compute similarity scores
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text_tokens)
    similarity = (image_features @ text_features.T).softmax(dim=-1)

# Print similarity scores
for desc, score in zip(text_descriptions, similarity[0]):
    print(f"Match Score for '{desc}': {score.item():.4f}")

from transformers import BlipProcessor, BlipForConditionalGeneration

# Load BLIP model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# Load the image
image = Image.open("generated_image.png")

# Generate a caption
inputs = processor(images=image, return_tensors="pt").to(device)
caption_ids = model.generate(**inputs)
caption = processor.decode(caption_ids[0], skip_special_tokens=True)

print(f"BLIP Caption: {caption}")

# Define a question about the image
question = "What is the animal doing?"

# Process inputs for VQA
inputs = processor(images=image, text=question, return_tensors="pt").to(device)
answer_ids = model.generate(**inputs)
answer = processor.decode(answer_ids[0], skip_special_tokens=True)

print(f"BLIP Answer: {answer}")

from transformers import BlipProcessor, BlipForQuestionAnswering
from PIL import Image
import torch

# Load the BLIP VQA model
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to(device)

# Load the image
image = Image.open("generated_image.png")

# Define a question about the image
question = "What is the animal doing?"

# Process inputs for VQA
inputs = processor(images=image, text=question, return_tensors="pt").to(device)
answer_ids = model.generate(**inputs)
answer = processor.decode(answer_ids[0], skip_special_tokens=True)

print(f"BLIP Answer: {answer}")